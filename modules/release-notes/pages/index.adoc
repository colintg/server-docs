= Release Notes
:description: Release notes for TigerGraph {page-component-version} LTS.
//:page-aliases: change-log.adoc, release-notes.adoc
:fn-preview: footnote:preview[Features in the preview stage should not be used for production purposes. General Availability (GA) versions of the feature will be available in a later release.]
:toc:
:toclevels:2

TigerGraph Server 3.10.0 LTS was released on Febuary 28th, 2024.

LTS versions are supported for 24 months from their initial release (X.X.0) and should be the choice for production deployments.

== Key New Features
=== 3.10.0
* xref:tigergraph-server:system-management:change-data-capture/cdc-overview.adoc[Change Data Capture (CDC)] - This equips TigerGraph users with the capability to automatically capture and stream data changes to external Kafka systems maintained by the user.
Additionally, CDC can be xref:gui:admin-portal:components/cdc.adoc[configured] in xref:gui:admin-portal:overview.adoc[Admin Portal].

* xref:tigergraph-server:system-management:workload-management.adoc#_workload_queue[Workload Queue] - Configure workload queues so that queries are routed to the appropriate queues during runtime.

* xref:tigergraph-server:data-loading:load-from-spark-dataframe.adoc[Spark Connector] - A new dedicated connector used with Apache Spark to read data from a Spark DataFrame and write to TigerGraph.

* xref:tigergraph-server:backup-and-restore:online-backup.adoc[] - Minimizes blocking time during backups, allowing successful execution of post requests for TigerGraph users.

* xref:tigergraph-server:backup-and-restore:differential-backups.adoc[] - Ensure data files that have changed since the most recently completed full backup are backed up without any data lost.

* [Add link] New non-interactive mode GSQL operations - Added four new non-interactive mode operations.

* [Add link] Externalizing KafkaConnect Config using Config Provider - TigerGraph’s Kafka connector config will use a reference to retrieve the config value from an external source.

* [Add link] Refined Upgrading Process - Changes to the upgrading process to allow a TigerGraph upgrade without having to upgrade the K8's operator.

* [Add link] Support Non-Interactive Upgrade - The user can use this option to avoid input ( y/n ) to switch to the new version. When it is given, the upgrade flow will switch to the new version automatically without input from the user.

* [Add link] Support Edge Accumulators - Support for edge accumulators for a single-hop distributed query.

* xref:gsql-ref:ddl-and-loading:modifying-a-graph-schema.adoc#_global_vs_local_schema_changes[Global Schema Restrict to Global Scope] - Users must have the global scope to interact with global schema change jobs (create, delete, run).

== Detailed List of New and Modified Features

=== TigerGraph Server
* xref:tigergraph-server:troubleshooting:audit-log.adoc[] - Audit logs maintain a historical record of activity events, noting the time, responsible user or service, and affected entity.

* xref:tigergraph-server:backup-and-restore:single-graph-import-export.adoc[] - Users can now import or export individually selected graphs.

* xref:tigergraph-server:cluster-and-ha-management:set-up-crr.adoc[Updates to the Set Up CRR] - Updates to the Cross-Region Replication (CRR) setup enable this feature to flow into the `gadmin backup restore` process preventing a writable gap.

=== GSQL Command and Querying Language

* xref:gsql-ref:querying:func/context-functions.adoc[] - Context functions are a set of new built-in functions that provide insights into the user's information and work inside `INSTALLED` queries, `INTERPRET` queries, and xref:3.10.1@tigergraph-server:user-access:rbac-row-policy/rbac-row-policy.adoc#_gsql_functions[GSQL Functions].

* xref:tigergraph-server:system-management:management-commands.adoc[Command Updates] - Added new flag `--local` to commands xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_start[gadmin start] and xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_stop [gadmin stop] reducing the time required to start/stop local services.

* GSQL Data Streaming xref:_improvements[] - Improvements to GSQL data streaming that reduce the CPU usage, improve performance, optimizes Disk usage, and increases stability and cohesion.

==== Loading

* xref:tigergraph-server:data-loading:load-from-warehouse.adoc[Support for Snowflake Data Warehouse] - Added support for loading data from Snowflake, another popular data warehouse.

* xref:tigergraph-server:data-loading:load-from-warehouse.adoc[Support for PostgreSql Data Warehouse] - Added support for loading data from PostgreSql, another popular data warehouse.

* xref:tigergraph-server:data-loading:avro-validation-with-kafka.adoc[] - The KafkaConnect feature flag `ErrorTolerance` enables data loading services to handle malformed data and report errors effectively

==== Schema

==== Querying and Query Management

=== Kubernetes Operator

=== Security
* xref:tigergraph-server:user-access:rbac-row-policy/row-policy-overview.adoc[RBAC: Row Policy] - is used to control access to specific rows of data in TigerGraph.
See also xref:tigergraph-server:user-access:rbac-row-policy/row-policy-ebnf.adoc[] for examples.

* xref:tigergraph-server:user-access:rbac-row-policy/rbac-row-policy.adoc#_object_based_privileges[Object-Based Privileges] - This mechanism allows users to grant or revoke privileges based on specific objects.
See xref:tigergraph-server:user-access:rbac-row-policy/row-policy-privlages-table.adoc[] for a complete list.

* xref:tigergraph-server:user-access:jwt-token.adoc[] - Provides token-based authentication in JSON web token (JWT) format, allows TigerGraph users better control over application access.

* xref:tigergraph-server:security:gsql-file-input-policy.adoc[] - `GSQL.fileInputPolicy` allows users to apply restrictions on the location of local files used to load data to TigerGraph.

* xref:tigergraph-server:data-loading:kafka-ssl-security-guide.adoc[Kafka Security via SSL] - Kafka brokers can be secured by SSL including the connections from Kafka clients to Kafka brokers.

== TigerGraph Suite Updates
=== Admin Portal

* xref:gui:admin-portal:components/cdc.adoc[Change Data Capture (CDC)] can be enabled in xref:gui:admin-portal:overview.adoc[Admin Portal].
* xref:gui:admin-portal:security/sso-oidc-okta.adoc[SSO.OIDC via Okta] - Support for Standard OIDC Authorization Code Flow for general purpose adds more security for logins to Admin Portal Users.


=== GraphStudio

* xref:gui:graphstudio:export-and-import-solution.adoc[Single Graph Import and Export Support]  - Allow users to choose a single graph and the data when they export or import data in GraphStudio.


=== TigerGraph Insights

* xref:insights:widgets:single-value.adoc[Changing Single Value Widget to Value Widget]  - Modified the value element of insights to support the mapping of multiple values.
* xref:insights:widgets:markdown-widget.adoc[Added Markdown Widget] - This addition allows users to add formatted text, links, images, and other rich content to the dashboards.
* [Add link] Added Scatter Chart Widget - The scatter chart will provide a visual representation of the relationship between two numerical variables, allowing users to identify patterns or correlations in the data.

== Fixed issues
=== Fixed and Improved in 3.10.0


==== Functionality
* Fixed issue where if the primary node is offline, access to Graph Studio was interrupted, but resumed once the primary node is back online (APPS-258)
* Fixed issue where some `GPR` and `Interpret` queries that specified the built-in `filter()` function would fail installation because of a row policy or tag filter (GLE-6448).


==== Crashes and Deadlocks

==== Improvements

* Improved by significantly reducing the CPU usage when a large number of loading jobs are started at the same time (TP-4159).
* Improved the write speed of loading jobs (TP-4159).
* Improved disk usage optimization by restricting a loading job in waiting status to only consumes disk resources when it actually writes data (TP-4474).
* Improved stability and cohesion of the connector and loader, which helps create better synchronization and reduces inconsistencies in the statuses (TP-4158).
* Improved significantly the pause time during backups from a few minutes to a couple of seconds, regardless of the data size. (CORE-3000).
* Improved data consistency during the backup and restore process (Core-3000).


==== Security

==== Performance

== Known Issues and Limitations

[cols="4", separator=¦ ]
|===
¦ Description ¦ Found In ¦ Workaround ¦ Fixed In

¦ Upgrading from a previous version of TigerGraph has known issues.
¦ 3.10.0
¦ See section xref:tigergraph-server:installation:upgrade.adoc#_known_issues_and_workarounds[Known Issues and Workarounds] for more details.
¦ TBD

¦ Input Policy feature has known limitations.
¦ 3.10.0
¦ See section xref:tigergraph-server:security:gsql-file-input-policy.adoc#_limitations[Input Policy Limitations] for more details.
¦ TBD

¦ Change Data Capture (CDC) feature has known limitations.
¦ 3.10.0
¦ See section xref:tigergraph-server:system-management:change-data-capture/cdc-overview.adoc#_cdc_limitations[CDC Limitations] for more details.
¦ TBD

¦ If the `FROM` clause pattern is a multi-hop and the `ACCUM` clause reads both primitive and container type attributes or accumulators of a vertex, the internal query rewriting logic may generate an invalid rewritten output.
¦ 3.9.3
¦ This results in the error message: `It is not allowed to mix primitive types and accumulator types in GroupByAccum`.
¦ TBD

¦ Users may see a high CPU usage caused by Kafka prefetching when there is no query or posting request.
¦ 3.9.3
¦ TBD
¦ TBD

¦ GSQL query compiler may report a false error for a valid query using a vertex set variable (e.g. `Ent` in `reverse_traversal_syntax_err`) to specify the midpoint or target vertex of a path in a FROM clause pattern.
¦ TBD
¦ TBD
¦ TBD

¦ If a loading job is expected to load from a large batch of files or Kafka queues (e.g. more than 500), the job’s status may not be updated for an extended period of time.
¦ 3.9.3
¦ In this case, users should check the loader log file as an additional reference for loading status.
¦ TBD

¦ When a GPE/GSE is turned off right after initiating a loading job, the loading job is terminated internally. However, users may still observe the loading job as running on their end.
¦ 3.9.3
¦ Please see xref:gsql-ref:ddl-and-loading:running-a-loading-job.adoc[Troubleshooting Loading Job Delays] for additional details.
¦ TBD

¦ For v3.9.1 and v3.9.2 when inserting a new edge in `GPR` and `INTERPRET` mode, the GPE will print out a warning message because a discriminator string is not set for new-inserted edges. Creating an inconsistent problem in delta message for GPR and `INTERPRET` mode.
¦ 3.9.2
¦ Please see xref:gsql-ref:ddl-and-loading:running-a-loading-job.adoc[Troubleshooting Loading Job Delays] for additional details.
¦ 3.9.3

¦ GSQL `EXPORT GRAPH` may fail and cause a GPE to crash when UDT type has a fixed STRING size.
¦ TBD
¦ TBD
¦ TBD

¦ After a global loading job is running for a while a fail can be encountered when getting the loading status due to `KAFKASTRM-LL` not being online, when actually the status is online.
Then the global loading process will exit and fail the local job after timeout while waiting the global loading job to finish.
¦ TBD
¦ TBD
¦ TBD

¦ When the memory usage approaches 100%, the system may stall because the process to elect a new GSE leader did not complete correctly.
¦ TBD
¦ This lockup can be cleared by restarting the GSE.
¦ TBD

¦ If the CPU and memory utilization remain high for an extended period during a schema change on a cluster, a GSE follower could crash, if it is requested to insert data belonging to the new schema before it has finished handling the schema update.
¦ TBD
¦ TBD
¦ TBD

¦ When available memory becomes very low in a cluster and there are a large number of vertex deletions to process, some remote servers might have difficulty receiving the metadata needed to be aware of all the deletions across the full cluster. The mismatched metadata will cause the GPE to go down.
¦ TBD
¦ TBD
¦ TBD

¦ Subqueries with SET<VERTEX> parameters cannot be run in Distributed or Interpreted mode.
¦ TBD
¦ (xref:3.9@gsql-ref:querying:operators-and-expressions.adoc#_subquery_limitations[Limited Distributed model support] is added in 3.9.2.)
¦ TBD

¦ Upgrading a cluster with 10 or more nodes to v3.9.0 requires a patch.
¦ 3.9
¦ Please contact TigerGraph Support if you have a cluster this large. Clusters with nine or fewer nodes do not require the patch.
¦ 3.9.1

¦ Downsizing a cluster to have fewer nodes requires a patch.
¦ 3.9.0
¦ Please contact TigerGraph Support.
¦ TBD

¦ During peak system load, loading jobs may sometimes display an inaccurate loading status.
¦ 3.9.0
¦ This issue can be remediated by continuing to run `SHOW LOADING STATUS` periodically to display the up-to-date status.
¦ TBD

¦ When managing many loading jobs, pausing a data loading job may result in longer-than-usual response time.
¦ TBD
¦ TBD
¦ TBD

¦ Schema change jobs may fail if the server is experiencing a heavy workload.
¦ TBD
¦ To remedy this, avoid applying schema changes during peak load times.
¦ TBD

¦ User-defined Types (UDT) do not work if exceeding string size limit.
¦ TBD
¦ Avoid using UDT for variable length strings that cannot be limited by size.
¦ TBD

¦ Unable to handle the tab character `\t` properly in AVRO or Parquet file loading. It will be loaded as `\\t`.
¦ TBD
¦ TBD
¦ TBD

¦ If `System.Backup.Local.Enable` is set to `true`, this also enables a daily full backup at 12:00am UTC.
¦ 3.9.0
¦ TBD
¦ 3.9.1

¦ The data streaming connector does not handle NULL values; the connector may operate properly if a NULL value is submitted.
¦ TBD
¦ Users should replace NULL with an alternate value, such as empty string "" for STRING data, 0 for INT data, etc.  (NULL is not a valid value for the TigerGraph graph data store.)
¦ TBD

¦ Automatic message removal is an Alpha feature of the Kafka connector. It has several xref:3.9@tigergraph-server:data-loading:load-from-cloud.adoc#_known_issues_with_loading[known issues].
¦ TBD
¦ TBD
¦ TBD

¦ The `DATETIME` data type is not supported by the `PRINT … TO CSV` statement.
¦ 3.9.0
¦ TBD
¦ 3.9.1

¦ The LDAP keyword `memberOf` for declaring group hierarchy is case-sensitive.
¦ TBD
¦ TBD
¦ TBD

|===

=== Compatibility Issues

[cols="2", separator=¦ ]
|===
¦ Description ¦ Version Introduced

¦ Users could encounter file input/output policy violations when upgrading a TigerGraph version.
See xref:tigergraph-server:security:gsql-file-input-policy.adoc#_backward_compatibility[Input policy backward compatibility.]
¦ v3.10.0

¦ When a PRINT argument is an expression, the output uses the expression as the key (label) for that output value.
To better support Antlr processing, PRINT now removes any spaces from that key. For example, `count(DISTINCT @@ids)` becomes `count(DISTINCT@@ids)`.
¦ v3.9.3+

¦ Betweenness Centrality algorithm: `reverse_edge_type (STRING)` parameter changed to `reverse_edge_type_set (SET<STRING>)`, to be consistent with `edge_type_set` and similar algorithms.
¦ v3.9.2+

¦ For vertices with string-type primary IDs, vertices whose ID is an empty string will now be rejected.
¦ v3.9.2+

¦ The default mode for the Kafka Connector changed from EOF="false" to EOF="true".
¦ v3.9.2+

¦ The default retention time for two monitoring services `Informant.RetentionPeriodDays` and `TS3.RetentionPeriodDays` has reduced from 30 to 7 days.
¦ v3.9.2+

¦ The filter for `/informant/metrics/get/cpu-memory` now accepts a list of ServiceDescriptors instead of a single ServiceDescriptor.
¦ v3.9.2+

a¦ Some user-defined functions (UDFs) may no longer be accepted due to xref:security:index.adoc#_udf_file_scanning[increased security screening].

* UDFs may no longer be called `to_string()`. This is now a built-in GSQL function.
* UDF names may no longer use the `tg_` prefix. Any user-defined function that began with `tg_` must be renamed or removed in `ExprFunctions.hpp`.
¦ v3.9+
|===

=== Deprecations

[cols="3", separator=¦ ]
|===
¦ Description ¦ Deprecated ¦ Removed

¦ The command `gbar` is removed and is no longer available.
However, if you are using a version of TigerGraph before 3.10.0 you can still use `gbar` to xref:tigergraph-server:backup-and-restore:gbar-legacy.adoc[create a backup with gbar] of the primary cluster.
See also xref:tigergraph-server:backup-and-restore:gbar-legacy.adoc[Backup and Restore with gbar] on how to create a backup.

¦ 3.7
¦ 3.10.0

¦ xref:tigergraph-server:user-access:vlac.adoc[Vertex-level Access Control (VLAC)] and xref:gsql-ref:querying:func/vertex-methods.adoc#_vlac_vertex_alias_methods_deprecated[VLAC Methods] are now deprecated and will no longer be supported.
¦ 3.10.0
¦ 4.0

¦ xref:tigergraph-server:data-loading:spark-connection-via-jdbc-driver.adoc[Spark Connection via JDBC Driver] is now deprecated and will no longer be supported.
¦ 3.10.0 
¦ TBD

¦ `Build Graph Patterns` is deprecated and will not be updated or supported and instead
we are focusing on xref:insights:widgets:index.adoc[Insights] as the tool of choice for building visual queries.
¦ v3.9.3
¦ TBD

¦ Kubernetes classic  mode (non-operator) is deprecated.
¦ v3.9
¦ TBD

¦ The `WRITE_DATA` RBAC privilege is deprecated.
¦ v3.7
¦ TBD
|===

== Release notes for previous versions
* xref:3.9@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.9]
* xref:3.8@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.8]
* xref:3.7@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.7]
* xref:3.6@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.6]
* xref:3.5@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.5]
* xref:3.4@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.4]
* xref:3.3@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.3]
* xref:3.2@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.2]
