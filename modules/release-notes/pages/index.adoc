= Release Notes
:description: Release notes for TigerGraph {page-component-version} LTS.
//:page-aliases: change-log.adoc, release-notes.adoc
:fn-preview: footnote:preview[Features in the preview stage should not be used for production purposes. General Availability (GA) versions of the feature will be available in a later release.]
:toc:
:toclevels:2

TigerGraph Server 3.10.0 LTS was released on Febuary 28th, 2024.

LTS versions are supported for 24 months from their initial release (X.X.0) and should be the choice for production deployments.

== Key New Features
=== 3.10.0
* xref:tigergraph-server:system-management:change-data-capture/cdc-overview.adoc[Change Data Capture (CDC)] - This equips TigerGraph users with the capability to automatically capture and stream data changes to external Kafka systems maintained by the user.

* xref:tigergraph-server:system-management:workload-management.adoc#_workload_queue[Workload Queue] - Configure workload queues so that queries are routed to the appropriate queues during runtime.

* xref:tigergraph-server:data-loading:load-from-spark-dataframe.adoc[Spark Connector] - A new dedicated connector used with Apache Spark to read data from a Spark DataFrame and write to TigerGraph.

== Detailed List of New and Modified Features

=== TigerGraph Server
* xref:tigergraph-server:troubleshooting:audit-log.adoc[] - Audit logs maintain a historical record of activity events, noting the time, responsible user or service, and affected entity.

=== GSQL Command and Querying Language

* xref:gsql-ref:querying:func/context-functions.adoc[] - Context functions are a set of new built-in functions that provide insights into the user's information and work inside `INSTALLED` queries, `INTERPRET` queries, and xref:3.10.1@tigergraph-server:user-access:rbac-row-policy/rbac-row-policy.adoc#_gsql_functions[GSQL Functions].

* xref:tigergraph-server:system-management:management-commands.adoc[Command Updates] - Added new flag `--local` to commands xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_start[gadmin start] and xref:tigergraph-server:system-management:management-commands.adoc#_gadmin_stop [gadmin stop] reducing the time required to start/stop local services.


==== Loading

* xref:tigergraph-server:data-loading:load-from-warehouse.adoc[Support for Snowflake Data Warehouse] - Added support for loading data from Snowflake, another popular data warehouse.

==== Schema

==== Querying and Query Management

=== Kubernetes Operator

=== Security
* xref:tigergraph-server:user-access:rbac-row-policy/row-policy-overview.adoc[RBAC: Row Policy] - is used to control access to specific rows of data in TigerGraph.
See also xref:tigergraph-server:user-access:rbac-row-policy/row-policy-ebnf.adoc[] for examples.

* xref:tigergraph-server:user-access:rbac-row-policy/rbac-row-policy.adoc#_object_based_privileges[Object-Based Privileges] - This mechanism allows users to grant or revoke privileges based on specific objects.
See xref:tigergraph-server:user-access:rbac-row-policy/row-policy-privlages-table.adoc[] for a complete list.

* xref:tigergraph-server:user-access:jwt-token.adoc[] - Provides token-based authentication in JSON web token (JWT) format, allows TigerGraph users better control over application access.

* xref:tigergraph-server:security:gsql-file-input-policy.adoc[] - `GSQL.fileInputPolicy` allows users to apply restrictions on the location of local files used to load data to TigerGraph.


== TigerGraph Suite Updates
=== Admin Portal

=== GraphStudio

=== GDS Library and ML Workbench

=== TigerGraph Insights

== Fixed issues
=== Fixed and Improved in 3.10.0


==== Functionality
* Fixed issue where if the primary node is offline, access to Graph Studio was interrupted, but resumed once the primary node is back online (APPS-258)
* Fixed issue where some `GPR` and `Interpret` queries that specified the built-in `filter()` function would fail installation because of a row policy or tag filter (GLE-6448).

==== Crashes and Deadlocks

==== Improvements

==== Security

==== Performance

== Known Issues

[cols="5", separator=¦ ]
|===
¦ Description ¦ Found In ¦ Workaround ¦ Fixed In ¦ Issue ID

¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD

¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD

¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD

¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD

¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD

¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD ¦ TBD


|===



* If the `FROM` clause pattern is a multi-hop and the `ACCUM` clause reads both primitive and container type attributes or accumulators of a vertex, the internal query rewriting logic may generate an invalid rewritten output.
Resulting in the error message: `It is not allowed to mix primitive types and accumulator types in GroupByAccum`.
* Users may see a high CPU usage caused by Kafka prefetching when there is no query or posting request.
* GSQL query compiler may report a false error for a valid query using a vertex set variable (e.g. `Ent` in `reverse_traversal_syntax_err`) to specify the midpoint or target vertex of a path in a FROM clause pattern.

* In 3.9.3, if a loading job is expected to load from a large batch of files or Kafka queues (e.g. more than 500), the job’s status may not be updated for an extended period of time. In this case, users should check the loader log file as an additional reference for loading status.
* In 3.9.3, when a GPE/GSE is turned off right after initiating a loading job, the loading job is terminated internally. However, users may still observe the loading job as running on their end. Please see xref:gsql-ref:ddl-and-loading:running-a-loading-job.adoc[Troubleshooting Loading Job Delays] for additional details.
* GSQL `EXPORT GRAPH` may fail and cause a GPE to crash when UDT type has a fixed STRING size.
* After a global loading job is running for a while a fail can be encountered when getting the loading status due to `KAFKASTRM-LL` not being online, when actually the status is online.
Then the global loading process will exit and fail the local job after timeout while waiting the global loading job to finish.
* *[FIXED 3.9.3]* For v3.9.1 and v3.9.2 when inserting a new edge in `GPR` and `INTERPRET` mode, the GPE will print out a warning message because a discriminator string is not set for new-inserted edges. Creating an inconsistent problem in delta message for GPR and `INTERPRET` mode.
* When the memory usage approaches 100%, the system may stall because the process to elect a new GSE leader did not complete correctly.
This lockup can be cleared by restarting the GSE.
* If the CPU and memory utilization remain high for an extended period during a schema change on a cluster, a GSE follower could crash, if it is requested to insert data belonging to the new schema before it has finished handling the schema update. 
* When available memory becomes very low in a cluster and there are a large number of vertex deletions to process, some remote servers might have difficulty receiving the metadata needed to be aware of all the deletions across the full cluster. The mismatched metadata will cause the GPE to go down.
* Subqueries with SET<VERTEX> parameters cannot be run in Distributed or Interpreted mode.
(xref:3.9@gsql-ref:querying:operators-and-expressions.adoc#_subquery_limitations[Limited Distributed model support] is added in 3.9.2.)
* Upgrading a cluster with 10 or more nodes to v3.9.0 requires a patch. Please contact TigerGraph Support if you have a cluster this large. Clusters with nine or fewer nodes do not require the patch. (This issue is fixed in 3.9.1)
* Downsizing a cluster to have fewer nodes requires a patch. Please contact TigerGraph Support.
* During peak system load, loading jobs may sometimes display an inaccurate loading status. This issue can be remediated by continuing to run `SHOW LOADING STATUS` periodically to display the up-to-date status.
* When managing many loading jobs, pausing a data loading job may result in longer-than-usual response time.
* Schema change jobs may fail if the server is experiencing a heavy workload. To remedy this, avoid applying schema changes during peak load times.
* User-defined Types (UDT) do not work if exceeding string size limit. Avoid using UDT for variable length strings that cannot be limited by size.
* Unable to handle the tab character `\t` properly in AVRO or Parquet file loading. It will be loaded as `\\t`.
* If `System.Backup.Local.Enable` is set to `true`, this also enables a daily full backup at 12:00am UTC (fixed in 3.9.1)
* The data streaming connector does not handle NULL values; the connector may operate properly if a NULL value is submitted. Users should replace NULL with an alternate value, such as empty string "" for STRING data, 0 for INT data, etc.  (NULL is not a valid value for the TigerGraph graph data store.)
* Automatic message removal is an Alpha feature of the Kafka connector.  It has several xref:3.9@tigergraph-server:data-loading:load-from-cloud.adoc#_known_issues_with_loading[known issues].
* The `DATETIME` data type is not supported by the `PRINT … TO CSV` statement (fixed in 3.9.1).
* The LDAP keyword `memberOf` for declaring group hierarchy is case-sensitive.

=== Compatibility Issues

[cols="2", separator=¦ ]
|===
¦ Description ¦ Version Introduced

¦ Users could encounter file input/output policy violations when upgrading a TigerGraph version.
See xref:tigergraph-server:security:gsql-file-input-policy.adoc#_backward_compatibility[Input policy backward compatibility.]
¦ v3.10.0

¦ When a PRINT argument is an expression, the output uses the expression as the key (label) for that output value.
To better support Antlr processing, PRINT now removes any spaces from that key. For example, `count(DISTINCT @@ids)` becomes `count(DISTINCT@@ids)`.
¦ v3.9.3+

¦ Betweenness Centrality algorithm: `reverse_edge_type (STRING)` parameter changed to `reverse_edge_type_set (SET<STRING>)`, to be consistent with `edge_type_set` and similar algorithms.
¦ v3.9.2+

¦ For vertices with string-type primary IDs, vertices whose ID is an empty string will now be rejected.
¦ v3.9.2+

¦ The default mode for the Kafka Connector changed from EOF="false" to EOF="true".
¦ v3.9.2+

¦ The default retention time for two monitoring services `Informant.RetentionPeriodDays` and `TS3.RetentionPeriodDays` has reduced from 30 to 7 days.
¦ v3.9.2+

¦ The filter for `/informant/metrics/get/cpu-memory` now accepts a list of ServiceDescriptors instead of a single ServiceDescriptor.
¦ v3.9.2+

a¦ Some user-defined functions (UDFs) may no longer be accepted due to xref:security:index.adoc#_udf_file_scanning[increased security screening].

* UDFs may no longer be called `to_string()`. This is now a built-in GSQL function.
* UDF names may no longer use the `tg_` prefix. Any user-defined function that began with `tg_` must be renamed or removed in `ExprFunctions.hpp`.
¦ v3.9+
|===

=== Deprecations

[cols="3", separator=¦ ]
|===
¦ Description ¦ Deprecated ¦ Removed

¦ xref:tigergraph-server:data-loading:spark-connection-via-jdbc-driver.adoc[Spark Connection via JDBC Driver] 
¦ 3.10.0 
¦ TBD

¦ `Build Graph Patterns` is deprecated and will not be updated or supported and instead
we are focusing on xref:insights:widgets:index.adoc[Insights] as the tool of choice for building visual queries.
¦ v3.9.3
¦ TBD

¦ Kubernetes classic  mode (non-operator) is deprecated.
¦ v3.9
¦ TBD

¦ The `WRITE_DATA` RBAC privilege is deprecated.
¦ v3.7
¦ TBD
|===

== Release notes for previous versions
* xref:3.9@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.9]
* xref:3.8@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.8]
* xref:3.7@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.7]
* xref:3.6@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.6]
* xref:3.5@tigergraph-server:release-notes:index.adoc[Release notes - TigerGraph 3.5]
* xref:3.4@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.4]
* xref:3.3@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.3]
* xref:3.2@tigergraph-server:release-notes:release-notes.adoc[Release notes - TigerGraph 3.2]
